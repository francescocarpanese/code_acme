{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.dm_control.utils import episode_data\n",
    "import numpy as np\n",
    "from dm_control.rl.control import Environment\n",
    "from environments.dm_control import moving_coil2D\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from IPython.display import Image,display\n",
    "from acme.wrappers.canonical_spec import CanonicalSpecWrapper\n",
    "from acme.wrappers.single_precision import SinglePrecisionWrapper\n",
    "from acme import specs\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from acme import specs\n",
    "from acme import types\n",
    "from acme.environment_loop import EnvironmentLoop\n",
    "from acme.wrappers.single_precision import SinglePrecisionWrapper\n",
    "from acme.wrappers.canonical_spec import CanonicalSpecWrapper\n",
    "from acme.agents.tf import dmpo, d4pg, mpo\n",
    "from acme.tf import networks\n",
    "from acme.tf import utils as tf2_utils\n",
    "from acme.utils import paths\n",
    "import dm_env\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "from dm_control.rl.control import Environment\n",
    "from environments.dm_control import tank, moving_coil, moving_coil2D\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\vec{s} = [\\vec{x}_{p}, \\vec{v}_{p}] = [\\vec{x}_{p}, \\frac{d \\vec{x}_{p}}{d t}]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\frac{\\vec{F}}{l} = \\sum_{i=0}^{N} \\frac{\\mu_0 I_p I_i}{2 \\pi} \\frac{ \\vec{x_p} - \\vec{x}_i}{||\\vec{x_p} - \\vec{x}_i||^2_2} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{d \\vec{s}}{d t} = \n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_p \\\\\n",
    "\\frac{1}{m} \\vec{F} ( \\vec{x}_p, I_0, ... , I_N; \\vec{x}_{c,0}, ...,  \\vec{x}_{c,N})\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free evolution of the system \n",
    "# Tests n time steps of environment with 0 action\n",
    "# Instantiate env\n",
    "environment = Environment(moving_coil2D.physics.Physics(),moving_coil2D.tasks.Step(debug = True), time_limit=2.)\n",
    "timestep = environment.reset()\n",
    "\n",
    "# Define null constant actions\n",
    "action = np.zeros(environment.action_spec().shape, np.float32)\n",
    "# Pull the coil \n",
    "action[0] = 1\n",
    "\n",
    "while not timestep.last():\n",
    "    timestep = environment.step(action)\n",
    "\n",
    "# Fetch sim data\n",
    "data = episode_data.pack_datadict(environment.task.datadict)\n",
    "\n",
    "moving_coil2D.make_movie.display_movie_ipynb(environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this tutorial just to be able to train the env.\n",
    "from distutils.log import debug\n",
    "from gym import spec\n",
    "from acme.utils.loggers import tf_summary\n",
    "\n",
    "time_limit = 2.\n",
    "\n",
    "physics = moving_coil2D.physics.Physics()\n",
    "task = moving_coil2D.tasks.Step(t_step = 1.)\n",
    "\n",
    "environment = Environment(physics, task, time_limit=time_limit)\n",
    "# Clip actions by bounds\n",
    "environment = CanonicalSpecWrapper(\n",
    "    environment= environment,\n",
    "    clip= True,\n",
    ")\n",
    "# Wrap to single precision\n",
    "environment = SinglePrecisionWrapper(environment)\n",
    "environment_spec = specs.make_environment_spec(environment)\n",
    "action_spec = environment_spec.actions\n",
    "\n",
    "\n",
    "policy_layer_sizes= (64,64)\n",
    "critic_layer_sizes= (64,64)\n",
    "\n",
    "\n",
    "# Get total number of action dimensions from action spec.\n",
    "num_dimensions = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "# Create the shared observation network; here simply a state-less operation.\n",
    "observation_network = tf2_utils.batch_concat\n",
    "\n",
    "\n",
    "# Create the policy network.\n",
    "policy_network = snt.Sequential([\n",
    "    networks.LayerNormMLP(policy_layer_sizes),\n",
    "    networks.MultivariateNormalDiagHead(num_dimensions),\n",
    "])\n",
    "\n",
    "\n",
    "# The multiplexer transforms concatenates the observations/actions.\n",
    "multiplexer = networks.CriticMultiplexer(\n",
    "    critic_network=networks.LayerNormMLP(critic_layer_sizes),\n",
    "    action_network=networks.ClipToSpec(action_spec))\n",
    "\n",
    "\n",
    "# Hack dimension to conform to mpo implementation\n",
    "critic_layer_sizes = list(critic_layer_sizes) + [1]\n",
    "critic_network = networks.CriticMultiplexer(\n",
    "    critic_network=networks.LayerNormMLP(critic_layer_sizes))\n",
    "\n",
    "\n",
    "agent = mpo.MPO(\n",
    "    environment_spec= environment_spec,\n",
    "    policy_network= policy_network,\n",
    "    critic_network= critic_network,\n",
    "    observation_network= observation_network,\n",
    "    batch_size = 40,\n",
    "    target_policy_update_period = 20,\n",
    "    target_critic_update_period = 20,\n",
    "    min_replay_size = 10,\n",
    ")\n",
    "\n",
    "\n",
    "outpath = '/content' # Destination of tensorboard log file\n",
    "logger = tf_summary.TFSummaryLogger(logdir = outpath)\n",
    "\n",
    "# Run the environment loop.\n",
    "loop = EnvironmentLoop(environment, agent, logger = logger)\n",
    "loop.run(200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "%tensorboard --logdir /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acme.agents.tf import actors\n",
    "# Create the actor.\n",
    "actor = actors.FeedForwardActor(\n",
    "    policy_network=policy_network)\n",
    "\n",
    "timestep = environment.reset()\n",
    "\n",
    "# Activate storing of data while looping\n",
    "environment.task.par_dict[\"debug\"] = True\n",
    "\n",
    "while not timestep.last():\n",
    "    action = actor.select_action(timestep.observation)\n",
    "    timestep = environment.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch sim data\n",
    "data = episode_data.pack_datadict(environment.task.datadict)\n",
    "\n",
    "moving_coil2D.make_movie.display_movie_ipynb(environment)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
