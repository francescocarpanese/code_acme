{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescocarpanese/code_acme/blob/main/tutorials/tank_control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CYYsHV8_OYv"
      },
      "source": [
        "# **Tutorial: train MPO agent to control a simple ode in continuous action space** \n",
        "\n",
        "![image](https://user-images.githubusercontent.com/91750384/148698219-754289f9-621f-4a0b-ac5a-4b64f260925b.png)\n",
        "\n",
        "In this tutorial we will show how to train an agent to control the level $h$ of a water tank.\n",
        "The physics environment is simulated with a simple ordinary differential equation described in the following sections and implemented respecting the [dm_control](https://github.com/deepmind/dm_control) framework requirements.\n",
        "\n",
        "The agent learns to control the inflow $w_{in}$ to follow a desired target $h$ in time. In this simple tutorial the agent observes directly the water level in the tank. The agent is trained using maximum a posteriori policy optimization ([mpo](https://arxiv.org/abs/1806.06920)). The deep reinforcment learning framework [acme](https://github.com/deepmind/acme), developed by DeepMind, is used to train the agent.\n",
        "\n",
        "The tutorial describes in order: \n",
        "- The environment components and  the implementation architecture required by [dm_control](https://github.com/deepmind/dm_control).\n",
        "- The agent set-up and training loop. \n",
        "- The evaluation of the trained agent when applied to the environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwXq_KfoWtfi"
      },
      "source": [
        "# Install code_acme\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grOgXVKMnxjj"
      },
      "outputs": [],
      "source": [
        "# Clone repo\n",
        "!git clone https://github.com/francescocarpanese/code_acme.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9HbWRWGoHSq"
      },
      "outputs": [],
      "source": [
        "cd code_acme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkDqwxALpD-O"
      },
      "outputs": [],
      "source": [
        "# Install code-acme packages. \n",
        "# The setup file includes the installation of acme, tensorflow and dm_control.\n",
        "!pip3 install virtualenv\n",
        "!virtualenv .code_acme\n",
        "!source .code_acme/bin/activate\n",
        "!pip install .[colab]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmyBOr3T5A-a"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "from environments.dm_control import tank\n",
        "from dm_control.rl.control import Environment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from acme import agents, specs\n",
        "from acme.environment_loop import EnvironmentLoop\n",
        "from acme.wrappers.single_precision import SinglePrecisionWrapper\n",
        "from acme.wrappers.canonical_spec import CanonicalSpecWrapper\n",
        "from acme.agents.tf import mpo\n",
        "from acme.tf import networks\n",
        "from acme.tf import utils as tf2_utils\n",
        "import sonnet as snt\n",
        "from acme.utils import paths\n",
        "import tensorflow as tf \n",
        "from acme.utils.loggers import tf_summary\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from dm_control.rl import control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4IpHUcX5NnL"
      },
      "source": [
        "#  **Water tank environment**\n",
        "\n",
        "## Physics model\n",
        "The ordinary differential equation governing the evolution of the water level $h$ in the tank, provided the appropriate physical scaling, can be written as,\n",
        "\\begin{align}\n",
        "\\frac{d h}{d t} = w_{out} + w_{in}.\n",
        "\\end{align}\n",
        "\n",
        "The following constitutive equation is assumed for the water outflow in the nozzle,\n",
        "\\begin{align}\n",
        "w_{out} = - \\alpha \\sqrt{h}.\n",
        "\\end{align}\n",
        "\n",
        "The equation is discretized in time with Euler explicit scheme,\n",
        "\\begin{align}\n",
        "h^{t+1} = dt_{sim}*(w_{out}^{t} + w_{in,t}^{t}) + h^{t}  \n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRyQish2m8Uq"
      },
      "source": [
        "## Environment in dm_control framework\n",
        "\n",
        "To make use of [acme](https://github.com/deepmind/acme) architecture for continuous control purposes it's convenient to implement the environment following the [dm_control](https://github.com/deepmind/dm_control) architecture. This allows to benefit from several tools and routines to simplify the set-up of the training.\n",
        "\n",
        "The [dm_control](https://github.com/deepmind/dm_control) framework requires to define the environment as a combination of a **physics** simular and one or multiple **tasks**. \n",
        "\n",
        "The package `code-acme` provides a more general implementation of the water tank environment, including extended capabilities to serialize the parameters and store the simulation data. However, given the explanatory purpose of this tutorial, we provide in the following a simplier minimal implementation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN9pnDUXT9xt"
      },
      "source": [
        "### physics\n",
        "The main component of the **physics** simulator, given the actions, is to step in time the ode. This is done in the `step` method.\n",
        "\n",
        "Numerical checks for the solution as well as sanity checks to avoid non physical state of the system are included in the `check_divergence` method.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWSX8DV8mcZL"
      },
      "outputs": [],
      "source": [
        "class Physics(control.Physics):\n",
        "    \"\"\"Water tank environment built on the dm_control.Environment class.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha: float, \n",
        "        dt_sim: float,\n",
        "        hmax: float,\n",
        "        init_state: [float],\n",
        "        ):\n",
        "        \"\"\"Initializes water tank\n",
        "\n",
        "        Attributes:\n",
        "            alpha: nozzle outflow coefficient\n",
        "            dt_sim:  [s] Discretization time interval for sim\n",
        "            hmax: [m] max water height in tank\n",
        "            init_state: [m] initial water height        \n",
        "        \"\"\"\n",
        "        self._alpha = alpha\n",
        "        self._dt_sim = dt_sim\n",
        "        self._h_max = hmax\n",
        "        self._init_state = init_state\n",
        "        self._state = self._init_state\n",
        "        self._time = 0.\n",
        "        self._action =  np.asarray([0.])\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets environment physics\"\"\"\n",
        "        self._state = self._init_state\n",
        "        self._time = 0.\n",
        "        self._action =  np.asarray([0.])\n",
        "\n",
        "    def after_reset(self):\n",
        "       pass\n",
        "\n",
        "    def step(self, n_sub_steps = 1):\n",
        "        \"\"\"Updates the environment according to the action\"\"\"\n",
        "\n",
        "        # Euler explicit time step\n",
        "        self._state = self._dt_sim*self._F() + self._state\n",
        "\n",
        "        # Update sim time\n",
        "        self._time += self._dt_sim\n",
        "\n",
        "        # Keep h min at 0\n",
        "        if self._state[0] <= 0.: self._state[0] = 0.\n",
        "\n",
        "    def _F(self):\n",
        "        \"\"\" Returns Physical RHS for ODE d state / dt = F(state, action) \"\"\"\n",
        "        return -self._alpha*np.sqrt(self._state) + self._action\n",
        "\n",
        "    def time(self):\n",
        "        \"\"\"Returns total elapsed simulation time\"\"\"\n",
        "        return self._time\n",
        "\n",
        "    def timestep(self):\n",
        "        \"\"\"Returns dt simulation step\"\"\"\n",
        "        return self._dt_sim\n",
        "\n",
        "    def check_divergence(self):\n",
        "        \"\"\" Checks physical terminations:\n",
        "         - water reached maximum level\n",
        "         - physical states not finite\n",
        "         \"\"\"\n",
        "        if  self._state[0] >= self_hmax:\n",
        "            raise control.PhysicsError(\n",
        "                f'h > max value = {self._hmax} [m]'\n",
        "            )\n",
        "\n",
        "        if not all(np.isfinite(self._state)):\n",
        "            raise control.PhysicsError('System state not finite')\n",
        "\n",
        "    def set_control(self, action):\n",
        "        \"\"\"Sets control actions\"\"\" \n",
        "        self._action = action \n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Returns physical states\"\"\"\n",
        "        return self._state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn9MX4dZWJTN"
      },
      "source": [
        "### task\n",
        "Each **task** has several purposes:\n",
        "\n",
        "-  Initialize the physics.\n",
        "\n",
        "For example, initializing the initial water level. This could differ from task to task.\n",
        "\n",
        "-  Define the reward function, hence the control target\n",
        "\n",
        "In this tutorial we will consider a Step target, which aims to keep the water level constant during a first time interval and then step the level to a different constant target. The reward function is defined as a normal distribution, with the mean equal to the target water level at a given time and the standard deviation $\\sigma$ set to $0.05m$. A well trained agent should be able to control the water level with a precision $\\sim \\sigma$. \n",
        "\n",
        "-  Provide the observations to be sent to the actor given the state of the system and the control target for the task.\n",
        "\n",
        "Each task could potentially observe a different subset of the system state. In this tutorial the agent will observe directly the water level, hence the full physical state. In order to let the agent learn how to deal with a time varying target, together with the physical state, we let the system observe the water level desired target.\n",
        "\n",
        "- Define physical limits for the actions.   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_GP5BVQmifv"
      },
      "outputs": [],
      "source": [
        "class Step(control.Task):\n",
        "    \"\"\" Step task:\n",
        "    Keep constant value and step to different constant value at t_step\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 maxinflow: float,\n",
        "                 h_goal1: float,\n",
        "                 h_goal2: float,\n",
        "                 t_step: float,\n",
        "                 precision: float,\n",
        "                 ):\n",
        "        \"\"\"Initialize Step task\n",
        "        \n",
        "        Parameters:\n",
        "            maxinflow: max control inflow\n",
        "            h_goal1: [m] target height 1st time interval\n",
        "            h_goal2: [m] target height 2nd time interval\n",
        "            t_step:  [s] switching time instant 1st->2nd target  \n",
        "            precision: [m] desired precision on h target   \n",
        "        \n",
        "        \"\"\"\n",
        "        self._maxinflow = maxinflow\n",
        "        self._h_goal1 = h_goal1\n",
        "        self._h_goal2 = h_goal2\n",
        "        self._t_step = t_step\n",
        "        self._precision = precision\n",
        "\n",
        "\n",
        "    def initialize_episode(self, physics):\n",
        "        \"\"\" Reset physics for the task \"\"\"\n",
        "        physics.reset()\n",
        "\n",
        "    def get_reference(self, physics):\n",
        "        \"\"\"Returns target reference\"\"\"\n",
        "        if physics.time() < self._t_step:\n",
        "            target = self._h_goal1\n",
        "        else:\n",
        "            target = self._h_goal2\n",
        "        return target\n",
        "\n",
        "    def get_observation(self, physics):\n",
        "        \"\"\"Returns specific observation for the task\"\"\"\n",
        "        # Let the actor observe the reference and the state\n",
        "        return np.concatenate((\n",
        "            [self.get_reference(physics)],\n",
        "            physics.get_state()\n",
        "            ))\n",
        "\n",
        "    def get_reward(self, physics):\n",
        "        \"\"\"Returns the reward given the physical state \"\"\"\n",
        "        sigma = self._precision\n",
        "        mean = self.get_reference(physics)\n",
        "        # Gaussian like rewards on target water level h\n",
        "        return np.exp(\n",
        "            -np.power(physics.get_state()[0] - mean, 2.)/(2*np.power(sigma, 2.))\n",
        "        )\n",
        "\n",
        "    def before_step(self, action, physics):\n",
        "        physics.set_control(action)\n",
        "     \n",
        "    def observation_spec(self, physics):\n",
        "        \"\"\"Returns the observation specifications\"\"\"\n",
        "        return specs.Array(\n",
        "            shape=(2,),\n",
        "            dtype=np.float32,\n",
        "            name='observation')\n",
        "\n",
        "    def action_spec(self, physics):\n",
        "        \"\"\"Returns the action specifications\"\"\"\n",
        "        return specs.BoundedArray(\n",
        "            shape=(1,),\n",
        "            dtype=np.float32,\n",
        "            minimum=0.,\n",
        "            maximum=self._maxinflow,\n",
        "            name='action')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVyH8KzpaoIY"
      },
      "source": [
        "## Simulate the environment with null actions\n",
        "We can now instantiate the environment and simulate it with null actions to get some intuition of the different components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox3Fk9gg0L90"
      },
      "source": [
        "Instance of `physics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "estuEgJoxqn_"
      },
      "outputs": [],
      "source": [
        "physics = Physics(\n",
        "    alpha= 1.0, # @param\n",
        "    dt_sim=0.05,  # @param\n",
        "    hmax=5,  # @param \n",
        "    init_state=[1.],  # @param\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgCPnIcC0OLr"
      },
      "source": [
        "Instance of `task`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fg9MCmey_2o"
      },
      "outputs": [],
      "source": [
        "task = Step(\n",
        "    maxinflow = 5., # @param\n",
        "    h_goal1 = 1., # @param\n",
        "    h_goal2 = 0.8, # @param\n",
        "    t_step = 1., # @param\n",
        "    precision = 0.05, # @param\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C5xJtkw0QfU"
      },
      "source": [
        "Instance of `environment`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k962-tCdatW3"
      },
      "outputs": [],
      "source": [
        "environment = Environment(\n",
        "    physics,\n",
        "    task,\n",
        "    time_limit=2.5, # @param\n",
        "    ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp2piCOcbSYX"
      },
      "source": [
        "Simulate with null action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz9J2rDc_NWv"
      },
      "outputs": [],
      "source": [
        "# Reset the environment\n",
        "TimeStep = environment.reset()\n",
        "    \n",
        "# Define constant 0 actions\n",
        "actions = np.zeros(environment.action_spec().shape, np.float32)\n",
        "\n",
        "# Simulate environment and store state,observation,reward,time\n",
        "s, o, r, t = [],[],[],[]\n",
        "while not TimeStep.last():\n",
        "  s.append(environment._physics._state)\n",
        "  o.append(TimeStep.observation.tolist())\n",
        "  r.append(TimeStep.reward)\n",
        "  t.append(environment.physics.time())\n",
        "  TimeStep = environment.step(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE1ChHGccAum"
      },
      "source": [
        "As expected the water level drops from the initial level $h = 1$ to $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt0a9IpDmNCr"
      },
      "outputs": [],
      "source": [
        "# Plot system state evolution\n",
        "plt.plot(t,s)\n",
        "plt.ylabel('h [m]')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWrpQcVBdMF2"
      },
      "source": [
        "We can check the observations in time that the task `Step` will provide to the agent. As you can see in the figure, the `target` level is provided to the observations together with the `state` to allow the agent learning to follow the target in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35c3vPeamqRq"
      },
      "outputs": [],
      "source": [
        "# Plot observation\n",
        "plt.plot(t,o)\n",
        "plt.ylabel('observations')\n",
        "plt.xlabel('time [s]')\n",
        "plt.legend(['target','state'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvByARCinayT"
      },
      "source": [
        "We can also check the time trace of the rewards provided by the task. Given the normal definition centered at the target, the reward drops while the water level gets further from the target. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nom9KzuPnQuX"
      },
      "outputs": [],
      "source": [
        "# Plot reward\n",
        "plt.plot(t,r)\n",
        "plt.ylabel('reward')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnEeXaCCqmJv"
      },
      "source": [
        "# **MPO Agent**\n",
        "\n",
        "In the following section we will show how to train an MPO agent for the environment and task. \n",
        "The section is taken mainly from the official [acme](https://github.com/deepmind/acme) tutorial in the following colab <a href=\"https://colab.research.google.com/github/deepmind/acme/blob/master/examples/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>. We invite to check the official tutorial for a more detailed explanation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSrni55Yhl_j"
      },
      "outputs": [],
      "source": [
        "# Set random seed for example reproducibility\n",
        "tf.random.set_seed(1500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOOgTwTGl5S9"
      },
      "source": [
        "First of all we make use of wrappers to wrap the environment in order to bound the allowed actions within specifications defined in task. We also cast the I/O of the environment into single precision. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75LSF_k7lteP"
      },
      "outputs": [],
      "source": [
        "# Clip actions by bounds\n",
        "environment = CanonicalSpecWrapper(environment= environment,clip= True) \n",
        "\n",
        "# Wrap to single precision\n",
        "environment = SinglePrecisionWrapper(environment) \n",
        "\n",
        "# Extract environment specifications\n",
        "environment_spec = specs.make_environment_spec(environment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASlIFQ8dm9Ld"
      },
      "source": [
        "## Set-up\n",
        "\n",
        "The next step is to set-up the NN used by the agent. MPO is a somewhat complicated algorithm, so we'll leave a full explanation of this method to the accompanying  [paper](https://arxiv.org/abs/1806.06920). \n",
        "\n",
        "However, we give here below some hints to understand the meaning of the following code. The `mpo` is an actor-critic algorithm. \n",
        "- The `actor` provides the actions given the state (of the agent). In this simple example the state of the agent consists directly on the observations, i.e. the state of the physical system (water level) and the target level. The actor contains the policy, which ultimately represents the control low to decide the inflow given the actor state (water level).  \n",
        "- The `critic` learns the state-action value function, which is related to the expected sum of future rewards, given a certain state and a taken action. This function provides intuitively the information on the value of taking a certain action being on a certain state. The critic is used during the learning process to update the policy. \n",
        "\n",
        "The `actor` and `critic` functions are approximated with NNs as [sonnet](https://github.com/deepmind/sonnet) MLP modules.\n",
        "\n",
        "MPO uses a distributional `actor`, as it can be seen from the `MultivariateNormalDiagHead`, which means that the policy obtained is not deterministic. The `policy_network` will return the mean and standard deviation of Normal distribution and the `actor` will sample from this distribution in order to get the actions to be applied to the environment. \n",
        "\n",
        "The [acme](https://github.com/deepmind/acme) architecture provides the possibility to specify a neural network for the observations. This is useful for example when dealing with observations coming from images to distil a simple agent state from pixel like information to be given to the critic. In this tuorial, the single physical state $h$ is directly observed and the observation network is simply an identity. \n",
        "\n",
        "The `multiplex` combines the actions and observations to be given as inputs to the critic network. \n",
        "\n",
        "Overall in general, for the mpo algorithm, one needs to specify 3 NNs for the `actor(policy)_net`, the `critic_net` and  the `observation_net`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC1JEOVsixYT"
      },
      "outputs": [],
      "source": [
        "# Get total number of action dimensions from action spec.\n",
        "num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)\n",
        "\n",
        "# Create the shared observation network; here simply a state-less operation.\n",
        "observation_network = tf2_utils.batch_concat\n",
        "\n",
        "# Specify default dimension for MLP\n",
        "policy_layer_sizes = [16,16]\n",
        "critic_layer_sizes = [16,16]\n",
        "\n",
        "# Create the policy network.\n",
        "policy_network = snt.Sequential([\n",
        "  networks.LayerNormMLP(policy_layer_sizes),\n",
        "  networks.MultivariateNormalDiagHead(num_dimensions),\n",
        "])\n",
        "\n",
        "critic_layer_sizes = list(critic_layer_sizes) + [1] # Hack to conform to mpo implementation\n",
        "# Create the critic network\n",
        "critic_network = networks.CriticMultiplexer(critic_network=networks.LayerNormMLP(critic_layer_sizes))\n",
        "\n",
        "# Pack agent networks\n",
        "agent_networks = {\n",
        "      'policy': policy_network,\n",
        "      'critic': critic_network,\n",
        "      'observation': observation_network,\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZivpE0xHfHl"
      },
      "source": [
        "Having specified the network architectures, we can finally define the agent, which combines the environment, the actor and the critic. Internally, the MPO agent contains the learner to update the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V1vu12EnEy_"
      },
      "outputs": [],
      "source": [
        "agent = mpo.MPO(\n",
        " environment_spec=environment_spec,\n",
        " policy_network=agent_networks['policy'],\n",
        " critic_network=agent_networks['critic'],\n",
        " observation_network=agent_networks['observation'], \n",
        " batch_size = 40,\n",
        " target_policy_update_period = 5,\n",
        " target_critic_update_period = 5,\n",
        " min_replay_size = 10,\n",
        " checkpoint = False,\n",
        ")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYnEQsDgiVKt"
      },
      "source": [
        "We define a tensorboard logger to store logs during training and inspect the learning curve afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD4NjhNiibMZ"
      },
      "outputs": [],
      "source": [
        "outpath = './' # Destination of tensorboard log file\n",
        "logger = tf_summary.TFSummaryLogger(logdir = outpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCyKR567iprB"
      },
      "source": [
        "## Training\n",
        "Finally we can train the agent. (With 200 episodes it will take ~2 min)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0IDqHJZiuDB"
      },
      "outputs": [],
      "source": [
        "num_episodes = 200 #@param\n",
        "\n",
        "# Run the environment loop.\n",
        "loop = EnvironmentLoop(environment, agent, logger = logger)\n",
        "\n",
        "# 350 is a good trainin\n",
        "loop.run(num_episodes=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHaTsyWqi6yy"
      },
      "source": [
        "# Visualize training logs with tensorboard\n",
        "\n",
        "It is convenient to visualize the training results in tensorboard. \n",
        "The main plot to observe is the episode return increase (~sum of the reward during an episode) over episodes, which tells how fast the agent is learning. \n",
        "\n",
        "Given that the simulation was set to last $2.5s$ with a $dt_{sim}$ of $0.05s$, we expect each episode to last $50$ time steps unless a physical limit defined in `physics` is hit, such as if the water exceed the maximum height.\n",
        "\n",
        "The maximum reward per time step is $1$, hence the maximum return per episode, given by the sum of the discounted rewards, is $\\sim50$. (discount factor is set to 0.99).\n",
        "\n",
        "We can see that the trained agent achieves an episode return of $\\sim 45$.\n",
        "\n",
        "Another interesting plots is the `StepsPerSecond` which tells how fast is the simulation of a single physical time step. Improving the speed of the environment allows to accelerate the collection of the experience for the learning process. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDwjsd7DBM6L"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/code_acme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-sc0QiLey9D"
      },
      "source": [
        "# Trained agent evaluation\n",
        "\n",
        "Now that we trained an agent and inspected its learning properties, we can use the policy obtained and evaluate how it performs in controlling the water level in the `tank`. \n",
        "\n",
        "First we simulate the environment with the trained policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqyFpeqAuHf6"
      },
      "outputs": [],
      "source": [
        "# Evaluate \n",
        "TimeStep = environment.reset()\n",
        "# Run episode and store system state, observation, action, reward and time\n",
        "s, o, a, r, t = [],[],[],[],[]\n",
        "while not TimeStep.last():\n",
        "  s.append(environment._physics._state)\n",
        "  o.append(TimeStep.observation.tolist())\n",
        "  r.append(TimeStep.reward)\n",
        "  t.append(environment.physics.time())\n",
        "  actions = agent.select_action(np.float32(TimeStep.observation)) \n",
        "  a.append(actions)\n",
        "  TimeStep = environment.step(actions)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkCcc628WiCA"
      },
      "source": [
        "We can now observe how the agent has learnt to control the $w_{in}$ to follow the target water level desired. We recall that the reward was designed as a Normal distribution centered on the target, and with $\\sigma = 0.05 m$. The trained policy achieved a tracking precision of $\\sim  0.05 m$ as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSXCEwMFNRWA"
      },
      "outputs": [],
      "source": [
        "# Plot observation\n",
        "plt.plot(t,o)\n",
        "plt.ylabel('observations')\n",
        "plt.xlabel('time [s]')\n",
        "plt.legend(['target','state'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS0DgJGcXqB5"
      },
      "source": [
        "The instantaneous rewards during the simulation with the trained agent are always close to the maximum value $=1$, except during the step instant. In this moment, the time decay of the water level is limited by the physical time scales of the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV-mpMLpNgRS"
      },
      "outputs": [],
      "source": [
        "# Plot reward\n",
        "plt.plot(t,r)\n",
        "plt.ylabel('reward')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68qkQW_qYQ0B"
      },
      "source": [
        "We can also investigate the actions produced by the policy. Since we used the `CanonicalSpecWrapper` to clip the actions by physical bounds specified in task, the trained policy will provide actions in the $[-1,1]$ canonical interval. We need therefore to transform the actions to convert the inflow in SI units in the $[0,maxinflow]$ interval. This convertion is done internally when using `EnvironmentLoop` during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8US021bPNmEJ"
      },
      "outputs": [],
      "source": [
        "# Revert action from canonical representation [-1,1] to SI \n",
        "f = lambda x: (np.clip(np.asarray(x),-1,1) + 1)*task._maxinflow/2 \n",
        "a = [f(x) for x in a]\n",
        "\n",
        "# Plot reward\n",
        "plt.plot(t,a)\n",
        "plt.ylabel('w_in')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMu8vIDtZYR0"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this tutorial we showed how to implement a simple physical ode environment  following the [dm_control](https://github.com/deepmind/dm_control) requirements. Then, we trained an mpo agent with [acme](https://github.com/deepmind/acme) framework to perform continuous action space control.\n",
        "\n",
        "Using deep reinforcement learning for this simple task and environment is obviously an overkill. However an extremely simple environment allows to play with deep reinforcement learning solutions at low computational costs. On top of that, making use of high quality frameworks such as [acme](https://github.com/deepmind/acme) allows potentially to easily scale up and generalize the approach."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Copy of tutorial_code_acme.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
